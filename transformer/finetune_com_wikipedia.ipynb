{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Com Wikipédia\n",
        "\n"
      ],
      "metadata": {
        "id": "fJOLIH9Lun8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descriçao\n",
        "\n",
        "---\n",
        "1. O objetivo deste notebook é aprender sobre Fine Tune\n",
        "2. Vamos descorrer sobre modelos de LLM e utilizar a biblioteca Torch para descorrer sobre assunto\n",
        "3. Vamos ensinar nossa máquina com os dados do [Wikipédia](https://pt.wikipedia.org/wiki/Ci%C3%AAncia)\n",
        "---"
      ],
      "metadata": {
        "id": "jKoQ7aZqu0S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baixando os pacotes\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SdlMTPrVvTZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad8cR6sGBwzE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentação\n",
        "\n",
        "1. **Transformers**:\n",
        "   - [Documentação do Hugging Face Transformers](https://huggingface.co/models?sort=trending)\n",
        "   - [Repositório GitHub](https://github.com/huggingface/transformers)\n",
        "\n",
        "2. **Requests**:\n",
        "   - [Documentação do Requests](https://docs.python-requests.org/en/latest/)\n",
        "   - [Repositório GitHub](https://github.com/psf/requests)\n",
        "\n",
        "3. **Datasets**:\n",
        "   - [Documentação do Hugging Face Datasets](https://huggingface.co/docs/datasets/)\n",
        "   - [Repositório GitHub](https://github.com/huggingface/datasets)\n",
        "\n",
        "4. **Torch**:\n",
        "   - [Documentação do PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "   - [Repositório GitHub](https://github.com/pytorch/pytorch)\n"
      ],
      "metadata": {
        "id": "iarcPeDX9VDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando as bibliotecas"
      ],
      "metadata": {
        "id": "tD3806zJvZKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBDDIbK3sBjF"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import requests\n",
        "from datasets import Dataset, load_dataset #esta biblioteca ajuda a carregar os dados\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importando o modelo"
      ],
      "metadata": {
        "id": "-30UbHJ-vi4v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbHELVnLsDF5"
      },
      "outputs": [],
      "source": [
        "model_name = \"pierreguillou/gpt2-small-portuguese\" #Carregamos o modelo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name) # criamos um tokenizador (PLN)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name) # Carregamos o modelo GP2, pre treinado"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requisitando o Wikipedia"
      ],
      "metadata": {
        "id": "rB8vkQhNv0xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://pt.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy\" # para fins de pesquisa, eh bom sempre pegar apenas uma unica pagina. Ao inves da api completa do Wikipedia\n",
        "response = requests.get(url)\n",
        "text_data = response.text\n",
        "text_data"
      ],
      "metadata": {
        "id": "tcLlsufPTdpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkqMeJ88Bog1"
      },
      "outputs": [],
      "source": [
        "\n",
        "lines = text_data.split('\\n')\n",
        "data_dict = {\"text\": lines}\n",
        "\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenização"
      ],
      "metadata": {
        "id": "b4rAshKwWEjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi2JO3bJsEan"
      },
      "outputs": [],
      "source": [
        "# tokenização do dataset\n",
        "def token_map(examples):\n",
        "    return tokenizer(examples[\"text\"], # extrai o nosso dataset\n",
        "                     padding=\"max_length\", #preenche sequencias até o comprimento máximo\n",
        "                     truncation=True, #Trunca, caso exceda o máximo de sequencias\n",
        "                     max_length=128) #define o cumprimento de cada token em 128\n",
        "\n",
        "tokenized_dataset = dataset.map(token_map, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "2q4EUD96YI_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ift_xMQguvHR"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset[400]['text'] #texto original do wikipedia\n",
        "#tokenized_dataset[200]['input_ids'] # ids do tokens do texto original.\n",
        "#okenized_dataset[100]['attention_mask'] # Lista de tokens que o modelo vai prestar atencao.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBtYSfP4sG9G"
      },
      "outputs": [],
      "source": [
        "# Preparando o lote de dados\n",
        "#Data Collector é um utilitário usado para preparar os dados durante a fase de treino\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, #aqui eu passo o tokenizador que criamos nas primeiras celulas\n",
        "                                                mlm=False) #mlm significa Masket Language Model, este parametro só é usado em casos onde o transformers nao usa mascara para treinar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuObRGPYsJKb"
      },
      "outputs": [],
      "source": [
        "# hiper parâmetros\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\", #diretorio de saida\n",
        "    overwrite_output_dir=True, #Sobreescrever o diretorio\n",
        "    num_train_epochs=1, #  Este epoch, ou epoca, eh apenas para fins de processamento e para apresentacao da aula.\n",
        "    per_device_train_batch_size=32, #tamanho do modelo em batchs\n",
        "    save_steps=10_000, #indica quantas etapas os modelos e arquivos serao salvos\n",
        "    save_total_limit=2 #limita o numero de checkpoints\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### criando o treinamento"
      ],
      "metadata": {
        "id": "HTGN1Z7wbj_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is_EzrhMsJNa"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model, #modelo de linguagem\n",
        "                  args=training_args, #os hiperparamentros\n",
        "                  data_collator=data_collator,  # o utilitario\n",
        "                  train_dataset=tokenized_dataset) #a customizacao"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinando"
      ],
      "metadata": {
        "id": "5GOrfi55bmeA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vEfiotTsJQN"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvando o modelo"
      ],
      "metadata": {
        "id": "c5hunfnVv6Nt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_ED4XrSsJSI"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"meu_guia\")\n",
        "tokenizer.save_pretrained(\"meu_guia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizando o Modelo"
      ],
      "metadata": {
        "id": "62sX5vd8v9-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#importando a biblioteca que criamos"
      ],
      "metadata": {
        "id": "MWEIoJciwBqg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Qqxx7FAVjj"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEbNExpyAmMu"
      },
      "outputs": [],
      "source": [
        "gerador = pipeline(\"text-generation\", model=\"meu_guia\")\n",
        "instrucao = \"\"\"Me fale sobre o Maycon\"\"\"\n",
        "\n",
        "resultado = gerador(instrucao,\n",
        "                    truncation=True,\n",
        "                    max_length=600, #maximo de comprimento do texto\n",
        "                    do_sample=True, #retorna uma amostragem estocagem, mais diversificada quando true\n",
        "                    temperature=0.9, #vai de zero a 1 e quanto maior, mais criativa fica o texto. tmb define a tal alucinacao\n",
        "                    top_k=50, # limita as proximas palavras ao numero k\n",
        "                    top_p=0.85#limita o conjunto de palavras para um novo grupo\n",
        "                   ) #ID do token de padding do GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zfO4f-rQSRz"
      },
      "outputs": [],
      "source": [
        "resultado = resultado[0]['generated_text']\n",
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indice_fim_texto_original = resultado.find(instrucao)\n",
        "novo_texto = resultado[indice_fim_texto_original + len(instrucao):].strip().replace('\\n\\n', ' ')\n",
        "novo_texto"
      ],
      "metadata": {
        "id": "MuVzETaLcjXL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "SdlMTPrVvTZ3",
        "tD3806zJvZKl",
        "-30UbHJ-vi4v",
        "rB8vkQhNv0xQ",
        "MWEIoJciwBqg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}